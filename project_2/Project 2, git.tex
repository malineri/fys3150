\documentclass[english,notitlepage]{revtex4-1}  % defines the basic parameters of the document
%For preview: skriv i terminal: latexmk -pdf -pvc filnavn



% if you want a single-column, remove reprint

% allows special characters (including æøå)
\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}

%% note that you may need to download some of these packages manually, it depends on your setup.
%% I recommend downloading TeXMaker, because it includes a large library of the most common packages.

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
\include{amsmath}
\usepackage{graphicx}         % include graphics such as plots
\usepackage{xcolor}           % set colors
\usepackage{hyperref}         % automagic cross-referencing (this is GODLIKE)
\usepackage{listings}         % display code
\usepackage{subfigure}        % imports a lot of cool and useful figure commands
\usepackage{float}
%\usepackage[section]{placeins}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{quantikz}
% defines the color of hyperref objects
% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ % this is just my personal choice, feel free to change things
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

%% Defines the style of the programming listing
%% This is actually my personal template, go ahead and change stuff if you want



%% USEFUL LINKS:
%%
%%   UiO LaTeX guides:        https://www.mn.uio.no/ifi/tjenester/it/hjelp/latex/
%%   mathematics:             https://en.wikibooks.org/wiki/LaTeX/Mathematics

%%   PHYSICS !                https://mirror.hmc.edu/ctan/macros/latex/contrib/physics/physics.pdf

%%   the basics of Tikz:       https://en.wikibooks.org/wiki/LaTeX/PGF/Tikz
%%   all the colors!:          https://en.wikibooks.org/wiki/LaTeX/Colors
%%   how to draw tables:       https://en.wikibooks.org/wiki/LaTeX/Tables
%%   code listing styles:      https://en.wikibooks.org/wiki/LaTeX/Source_Code_Listings
%%   \includegraphics          https://en.wikibooks.org/wiki/LaTeX/Importing_Graphics
%%   learn more about figures  https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions
%%   automagic bibliography:   https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management  (this one is kinda difficult the first time)
%%   REVTeX Guide:             http://www.physics.csbsju.edu/370/papers/Journal_Style_Manuals/auguide4-1.pdf
%%
%%   (this document is of class "revtex4-1", the REVTeX Guide explains how the class works)


%% CREATING THE .pdf FILE USING LINUX IN THE TERMINAL
%%
%% [terminal]$ pdflatex template.tex
%%
%% Run the command twice, always.
%% If you want to use \footnote, you need to run these commands (IN THIS SPECIFIC ORDER)
%%
%% [terminal]$ pdflatex template.tex
%% [terminal]$ bibtex template
%% [terminal]$ pdflatex template.tex
%% [terminal]$ pdflatex template.tex
%%
%% Don't ask me why, I don't know.

\begin{document}

\title{Project 2}      % self-explanatory
\author{Malin Eriksen}          % self-explanatory
\date{\today}                             % self-explanatory
\noaffiliation                            % ignore this, but keep it.


\maketitle 
    
\textit{https://github.com/malineri/fys3150/tree/main/project\_2}
    
\section*{Problem 1}
In this project we will be looking at a horizontal beam. The beam will be of length L, and a force F will be applied in the endpoint of this beam. This will be described by the second-order differential equation (1).
\begin{equation}\label{eq:newton}
    \gamma \frac{d^2 u(x)}{dx^2} = -F u(x).
\end{equation}
We begin with scaling this formula into a dimensionless equation, by changing the x to a unitless variable $\hat{x} = \frac{x}{L}$. The derivation term then becomes, 
\begin{align*}
\frac{d}{dx} = \frac{d\hat{x}}{dx} \frac{d}{d\hat{x}} = \frac{1}{L} \frac{d}{d \hat{x}}.
\end{align*}
We have a second order differential equation, so we have to do this operation twice. 
Adding this to equation(1) we get the formula,
\begin{align*}
 \frac{\gamma}{L^2} \frac{d^2 u(\hat{x})}{d\hat{x}^2} = -F u(\hat{x}).
\end{align*}
Which we rearrange slightly, 
\begin{align*}
\frac{d^2 u(\hat{x})}{d\hat{x}^2} = - \frac{F L^2}{\gamma} u(\hat{x}).
\end{align*}
Now we introduce a new variable lambda $\lambda = \frac{F L^2}{\gamma}$ then we find that the dimensionless equation of our first equation can be written as, 
\begin{equation}\label{eq:newton}
    \frac{d^2 u(\hat{x})}{d\hat{x}^2} = - \lambda u(\hat{x}).
\end{equation} 
\newpage{}



\section*{Problem 2}\
\\
To solve this problem we will be using matrices. We write a short program to set up a tridiagonal NxN matrix A, when N = 6. We want this matrix to solve the classic eigenvector, eigenvalue problem $\mathbf{A} \vec{v} = \lambda \vec{v}$ in the same way we formulated our dimensionless equation(2).\footnote{https://stackoverflow.com/questions/70556590/how-to-solve-for-eigen-value-of-a-matrix-numerically} We being with creating an algorithm with a general function that determines a tridiagonal matrix. This algorithm we see in Algorithm 1.
\begin{algorithm}[H]
    \caption{Creating function that takes values from diagonal and makes a tridiagonal matrix.}\label{av_lik_lv}
    \begin{algorithmic}
    	\State int main()\{
	\State arma::mat create tridiagonal(int n, double a, double d, double e)
	\State arma::mat A = arma::mat(n, n, arma::fill::eye); \Comment{n x n identity matrix}
        \State int N = 6  \Comment{We begin with setting the length of the matrix N = 6}
        \State A(0, 0) = d; A(1, 0) = e; A(0, 1) = a;  \Comment{Manually filling in values of A, Could potentially be done more efficiantly in a loop.}
 	\State return A;
	\State \}
    \end{algorithmic}
\end{algorithm}
We now want to put in values of N, a, d and e, to determine our exact values. We want those to be a, $e = -1/h^2$ and $d = -2/h^2$. And we also want the program to print the eigenvalues and eigenvectors. A program returning the matrix, its eigenvalue and its corresponding eigenvectors is written in Algorithm 2.
\begin{algorithm}[H]
    \caption{Our main containing the values of our matrix, and printing the matrix, its eigenvector and eigenvalues.}\label{av_lik_lv}
    \begin{algorithmic}
    	\State int main()\{
	\State int N = 6.; float h = 1.; float a = (-1.)/(h*h); float d = (2.)/(h*h); \Comment{Setting values we use in the matrix}
	\State arma::mat A = create tridiagonal(N, a, d, a); \Comment{Create matrix A from function}
        \State int N = 6  \Comment{We begin with setting the length of the matrix N = 6}
        \State arma::vec eigval;
        \State arma::mat eigvec;
        \State arma::eig sym(eigval, eigvec, A);
        \State int width = 18; int prec = 10; \Comment{Parameters for output formatting}
        \State std::cout $<<$ ”\#” $<<$ std::setw(width-1) $<<$ A
        \State $<<$ std::endl;
        \State std::cout $<<$ ”\#” $<<$ std::setw(width-1) $<<$ eigvec
        \State $<<$ std::endl;
        \State std::cout $<<$ ”\#” $<<$ std::setw(width-1) $<<$ eigval
        \State $<<$ std::endl;
 	\State return 0;
	\State \}
    \end{algorithmic}
\end{algorithm}
Now that we have found the eigenvalues and corresponding eigenvectors we print the results, to make it easier to compare the analytical and numerical solutions we remove the /h2 part of the a, d and e values, so that we use the simple tridiagonal matrix with 2 on the diagonal and -1 on the upper and lower diagonal. This gives us the results we can see in table(1).
\begin{table}%[h!]
    \centering
    \caption{Eigenvalues and corresponding eigenvectors of a tridiagonal matrix A}
    \begin{tabular}{c@{\hspace{1cm}} c}
        \hline
        Eigenvalues $\lambda$ & Eigenvectors $\vec{v}$ \\
        \hline
        0.1981 & [0.2319, 0.4179, 0.5211, 0.5211, 0.4179, 0.2319] \\
        0.7530 &  [-0.4179, -0.5211, -0.2319, 0.2319, 0.5211, 0.4179] \\
        1.5550 & [0.5211, 0.2319, -0.4179, -0.4179, 0.2319, 0.5211]\\
        2.4450 &  [ 0.5211, -0.2319, -0.4179, 0.4179, 0.2319, -0.5211]\\
        3.2470 &  [0.4179, -0.5211, 0.2319, 0.2319, -0.5211, 0.4179]\\
        3.8019 & [-0.2319, 0.4179, -0.5211, 0.5211, -0.4179, 0.2319]\\
        \hline
    \end{tabular}\label{tab:output_table}
\end{table}
We can check if these values are correct by comparing the results to the analytical results with the formulas
\begin{align*}
\lambda^{i} = d + 2a \cos \bigg(\frac{i \pi}{N + 1} \bigg) 
\end{align*}
\begin{align*}
\vec{v}^{i} = \bigg[ \sin \bigg(\frac{i \pi}{N + 1} \bigg), \sin \bigg( \frac{2i \pi}{N + 1} \bigg), ... , \sin \bigg(\frac{Ni \pi}{N + 1} \bigg) \bigg]^T 
\end{align*}
Where i = 1, ... , N. 
We make a program that solves this algorithm for our N = 6 case. This program is called \textit{analytical 2.py}, and the algorithm is added to the github repo. We get the same values on the eigenvalues, but we get different eigenvectors, something that tells us there must be a small mistake in one of the programs. If time I will go back and look at potential mistakes in the program. In table(2) we see the values we get from the analytical solution.
\begin{table}%[h!]
    \centering
    \caption{Eigenvalues and corresponding eigenvectors of a tridiagonal matrix A}
    \begin{tabular}{c@{\hspace{1cm}} c}
        \hline
        Eigenvalues $\lambda$ & Eigenvectors $\vec{v}$ \\
        \hline
        0.1981 & [0.43388374, -0.78183148, 0.97492791, -0.97492791,0.78183148, -0.43388374]\\
        0.7530 &  [0.78183148, -0.97492791, 0.43388374, 0.43388374, -0.97492791, 0.78183148]\\
        1.5550 & [0.97492791, -0.43388374, -0.78183148, 0.78183148, 0.43388374, -0.97492791]\\
        2.4450 &  [0.97492791, 0.43388374, -0.78183148, -0.78183148, 0.43388374, 0.97492791]\\
        3.2470 &  [0.78183148, 0.97492791, 0.43388374, -0.43388374, -0.97492791, -0.78183148]\\
        3.8019 & [0.43388374, 0.78183148, 0.97492791, 0.97492791, 0.78183148, 0.43388374]\\
        \hline
    \end{tabular}\label{tab:output_table}
\end{table}

 



\section*{Problem 3}
\subsection*{a)}\
\\
In general we would like to be able to take any matrix and find the operations needed to create this into a identity matrix using the Jacobi rotation method. And then later from this method finding the eigenvectors and values of the matrix. To be able to create a program that uses the Jacobi rotation method we first write a program that takes a matrix and finds the biggest off diagonal element. The algorithm(3) is used for this function.
\begin{algorithm}[H]
    \caption{Creating function that finds the biggest off diagonal element in a matrix A}\label{av_lik_lv}
    \begin{algorithmic}
    	\State double max\_offdiag\_symmetric(const arma::mat\& A)\{
	\State double m = INT\_MIN;
	\State for (int i = 0; i $<$ A.n\_rows; i++)\{
        \State for (int j = 0; j $<$ i; j++)\{
        \State if (abs(A(i, j)) $>$ m)\{
 	\State m = A(i, j);
	\State \}
	\State \}
	\State \}
	\State return m;
	\State \}
    \end{algorithmic}
\end{algorithm}
\subsection*{b)}\
\\
Now we apply this function to a known matrix to see if we get the output we are looking for. We use the matrix A from equation(3). 
\begin{equation}
A = \begin{bmatrix}
1 & 0 & 0 & 0.5\\
0 & 1 & -0.7 & 0\\
0 & -0.7 & 1 & 0\\
0.5 & 0 & 0 & 1\\
\end{bmatrix}
\end{equation}
We check only under the diagonal for our program since our matrix is symmetric over and under the diagonal. We know already from our matrix that the biggest absolute value is 0.7, and that the biggest plus value is 0.5. In our program we have the print of 0.5, unless we take the absolute value of the matrix. If we do so we find the greatest value to be 0.7. This we can test with other matrices aswell, and if it turnes out that the matrix has a negative value that is bigger than the positive value we may change the algorithm(3) to contain $m = A(A_n.rows - 1, 0)$, to have the biggest negative value. Otherwise we will have the same result in eigenvalues and vectors, only that this will also then be the absolute value. The algorithm implementing the matrix to the function above is represented in algorithm(4). 
\begin{algorithm}[H]
    \caption{Our main containing the values of our matrix, and printing the matrix, its eigenvector and eigenvalues.}\label{av_lik_lv}
    \begin{algorithmic}
    	\State int main()\{
	\State int N = 4; \Comment{Setting values we use in the matrix}
	\State arma::mat A = arma::mat(N, N, arma::fill::eye);
        \State A(0, 3) = 0.5; A(1, 2) = -0.7; A(2, 1) = -0.7; A(3, 0) = 0.5; \Comment{filling in the exact matrix elements}
        \State arma::mat B = abs(A); \Comment{taking the absolute value of the matrix, otherwise the element -0.7 will not be counted}
        \State double max\_value = max\_offdiag\_symmetric(B);
        \State std::cout<< max\_value;
 	\State return 0;
	\State \}
    \end{algorithmic}
\end{algorithm}

\section*{Problem 4}
\subsection*{a)}\
\\
The Jacobi rotation method is a method where diagonalize A, so that we have the equation $S^T A S = D$. The method is based on doing a series of transformations until we end up with A being something close enough to the diagonal, D. The number of iterations we have to do will then be described as m. And for each step we apply the $S^T$ and S for the step before M-1. In general this can be described by equation(4).
\begin{equation}
S_{M-1}{T} ... S_2^T S_1^T A S_1S_2 ... S_{M-1} = A^{m} = D
\end{equation}
The elements in the S vector will then be the eigenvectors of A, and the diagonal D will contain the eigenvalues. Rewriting this to an algorithm we will use equation(5).
\begin{equation}
A^{m + 1} = S_M^T A^{(M)} S_M
\end{equation}
The matrix that does the rotation is the $S_M$ matrix, this will rotate the matrix clockwise at four important points $(k, k) = \cos \theta$, $(k, l) = \sin \theta$, $(l, k) = -\sin \theta$ and $(l, l) = \cos \theta$. These points will rotate the matrix to make our biggest off-diagonal value in A go to zero. The algorithm we make will find the biggest value, do a rotation, then search for a new biggest value, do another rotation, and do this on repeat till we are close enough to the diagonal matrix. See equation(6). 
\begin{equation}
S_M = \begin{bmatrix} 
1 & & & & \\
 & & \cos \theta & \sin \theta & \\
 & & -\sin \theta & \cos \theta & \\
 & & & & & 1
 \end{bmatrix}
\end{equation} 
We will also introduse the vector R, that together with $S_M$ creates the span of eigenvectors. The first element of R is the identity matrix. 

\begin{align}
R^{[M]} = R^{[M - 1]} S_{M - 1}= I S_1, S_2, ... S_{M - 1}
\end{align}


From this matrix we can put up a set of equations, for the different elements in the matrix A, that we can use to make the algorithm.
\begin{align}
A^{[2]}(l, l) &= A^{[1]}(l, l) \cos^2 \theta - 2 A(k, l) \cos \theta \sin \theta + A^{[1]}(k, k) \sin^2 \theta \\
A^{[2]}(k, k) &= A^{[1]}(k, k) \cos^2 \theta + 2 A(k, l) \cos \theta \sin \theta + A^{[1]}(l, l) \sin^2 \theta \\
A^{[2]}(k, l) &= (A^{[1]}(l, l) - A^{[1]}(k, k))\cos \theta \sin \theta + A^{[1]}(k, l)(\cos^2 \theta - \sin^2 \theta) \rightarrow 0.
\end{align}
We reqire the last equation to be zero, and from these equations we get the formula, 
\begin{align}
\bigg( \frac{A^{[1]}(l, l) - A^{[1]}(k, k)}{A^{[1]}(k, l)} \bigg)\cos \theta \sin \theta + \cos^2 \theta - \sin^2 \theta = 0
\end{align}
To make the algorithm simpler we introduce the notation $\tan \theta = t = \frac{s}{c}$ , $\tau = \frac{A(k, k) - A(l, l)}{2(k, l)}$. A is a symmetric matrix, which means the element (k, l) = (l, k). This gives us, 
\begin{align}
\sin^2 \theta + 2\epsilon \cos \theta \sin \theta - \cos^2 \theta &= 0 \\
t^2 + 2\epsilon t - 1 &= 0.\\
 -\tau \sqrt{1 + \tau^2} &= t
\end{align}
From this we can compute $c = \frac{1}{\sqrt{1 + t^2}}$ and $s = ct$. And from all this we will now be able to compute $A^{[2]}(l, l)$ and $A^{[2]}(k, k)$. And we can make loops of iterations to do this for the next orders as well. This Algorithm is slightly longer, so I will leave this program in the github repo with the name \textit{problem\_4.cpp}. 


\subsection*{b)}\
\\
If I would be able to make this program work I would be able to do it for a matrix with N = 6. And then I would analyse the results we get from the jacobi rotation method with the analytical results we got from problem 2. I would also see how it compares to the eigenvectors and eigenvalues we found from the armadillo implementation in problem 2. 




\section*{Problem 5}
\subsection*{a)}\
\\
Whenever we have made the program above function, we may use it to figure out the number of iterations needed for a specific matrix before it has reached an almost diagonal matrix. We can run the program with different sizes on the matrix. We choose N = 10, 100 and 1000 for example, and then we see the number of iterations corresponding to this before out matrix becomes close to diagonal. This number is expected to increase more than linearly with N. This is because the rotation matrix will rotate one number at the time, and when rotating, another number that has previous been rotated and now is zero, may be rotated back, and now again be some value. This means that the greater the matrix, the higher the possibility to change back an already fixed number will increase, and therefore the number of iterations will increase more compared to the increasing of the size of the matrix itself. I did not get my program working in the previous task, and therefore making a plot or table with the numbers for these two will be hard to find. But I have just made some random guesses to be able to make some sort of a plot of something that may be something close to the actual graph. The numbers I have guessed, and used in the plot is possible to see in table(3)
\begin{table}[h!]
    \centering
    \caption{Number of iterations/tranformations before the matrix D is close to diagonal, for different sized A NxN matrices.}
    \begin{tabular}{c@{\hspace{1cm}} c}
        \hline
        Iterations [i]& Size of matrix [N]\\
        \hline
        10 & 100\\
        20 & 1000\\
        50 & 3000\\
        100 & 15000\\
        500 & 50000\\
        1000 & 1000000\\
        \hline
    \end{tabular}\label{tab:output_table}
\end{table}
Most probably the matrix A is not as big as N here, but probably the number of transformations, or iterations in the program, may be even bigger for smaller N, this may end up giving the program troubles and making it a slow one. In figure(1) we see the plot of this table. Since I had very little values the plot is not very smooth, but it gives an indication of what I thought.
\begin{figure}[h!]
    \centering %Centers the figure
    \includegraphics[scale=0.55]{IN.pdf} %Imports the figure.
    \caption{Plot of the values descrived in table(3).}
    \label{fig:rel_err}
\end{figure}


\subsection*{b)}\
\\
From Wikipedia I find that the definition of a dense matrix is a matrix where most of the elements are non-zero. Which means that we would have to have even more iterations in the method before we are able to end up with a diagonal matrix. 


\section*{Problem 6}
\subsection*{a)}
With the discretized x we will use n = 10 steps, which means that our matrix A will be of size n-1, when we remove the endpoints. We will then use the jacobi iteration to make a plot of the three eigenvectors corresponding to the three lowest eigenvalues. From our S matrix we then find the eigenvectors S[i] corresponding to the lowest eigenvalues that we find in the diagonal D. We can then make a program that finds the lowest value in the matrix D, and prints the column and row. From this information we find the corresponding x values. And then we can plot the x against the vectors F[i] for the 3 lowest values. 


\subsection*{b)}
We can repeat the same operation with changing n = 10 steps to n = 100 steps. 





















   
\end{document}
